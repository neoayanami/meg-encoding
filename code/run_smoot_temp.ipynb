{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import mne\n",
    "import imageio\n",
    "from collect_data import *\n",
    "from collect_metrics import get_topomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimuli_path = meg_path + '/stimuli/audio'\n",
    "patient = ['01']\n",
    "sub_decim = 1\n",
    "brain_signal_data = []\n",
    "audio_signal_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in tqdm(patient):\n",
    "    print('PATIENT: ', subject)\n",
    "    for sess in range(len(session)):\n",
    "        print(\"SESSION: \", session[sess])\n",
    "        for story in task_list:\n",
    "            print('AUDIO_NAME: ', story)\n",
    "            selected_sound_ids = tasks_with_sound_ids[story]\n",
    "            story_uid = int(task[story])\n",
    "            print(\"STORY_UID: \", story_uid)\n",
    "            raw = get_bids_raw(meg_path, subject, session[sess], str(story_uid))\n",
    "            for z, sound_id in enumerate(selected_sound_ids):\n",
    "                print(\"SOUND_ID: \", float(sound_id))\n",
    "                epochs_data = get_epochs(raw, float(story_uid), float(sound_id), sub_decim)\n",
    "                # save_data(meg_spectr_ranged, 'megsp', subject, str(i), str(story_uid), audio_name, str(z))\n",
    "                if subject == '01':\n",
    "                    audio_path = os.path.join(stimuli_path, f'{story}_{z}.wav')\n",
    "                    audio_spectr = get_audio_spectrogram(audio_path, epochs_data)\n",
    "                    print('AUDIO_SPECTR_SHAPE: ', audio_spectr.shape)\n",
    "                    audio_signal_data.append(audio_spectr)\n",
    "                    save_data(audio_spectr, 'audio_nfft', 'audio', '_', str(story_uid), story, str(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION_AUDIO_TENSOR_TRAIN:  torch.Size([11958, 4097, 24])\n",
      "DIMENSION_AUDIO_TENSOR_VALID:  torch.Size([1684, 4097, 24])\n",
      "DIMENSION_AUDIO_TENSOR_TEST:  torch.Size([3480, 4097, 24])\n",
      "DIMENSION_MEG_TENSOR_TRAIN:  torch.Size([11958, 208, 16, 26])\n",
      "DIMENSION_MEG_TENSOR_VALID:  torch.Size([1684, 208, 16, 26])\n",
      "DIMENSION_MEG_TENSOR_TEST:  torch.Size([3480, 208, 16, 26])\n"
     ]
    }
   ],
   "source": [
    "megsp_path = os.path.join(meg_path, 'collect_data/megsp')\n",
    "audio_path = os.path.join(meg_path, 'collect_data/audio_nfft')\n",
    "megsp_list = os.listdir(megsp_path)\n",
    "audio_list = os.listdir(audio_path)\n",
    "\n",
    "select_subj = \"02\"\n",
    "megsp_list_session_0 = [f for f in megsp_list if f.startswith(select_subj) and f.split('_')[1] == '0']\n",
    "megsp_list_session_1 = [f for f in megsp_list if f.startswith(select_subj) and f.split('_')[1] == '1']\n",
    "\n",
    "audio_tensor_train, audio_tensor_valid, audio_tensor_test = get_splitted_tensor(audio_list, audio_path)\n",
    "audio_tensor_train = torch.cat((audio_tensor_train, audio_tensor_train), 0)\n",
    "audio_tensor_valid = torch.cat((audio_tensor_valid, audio_tensor_valid), 0)\n",
    "audio_tensor_test = torch.cat((audio_tensor_test, audio_tensor_test), 0)\n",
    "print('DIMENSION_AUDIO_TENSOR_TRAIN: ', audio_tensor_train.shape)\n",
    "print('DIMENSION_AUDIO_TENSOR_VALID: ', audio_tensor_valid.shape)\n",
    "print('DIMENSION_AUDIO_TENSOR_TEST: ', audio_tensor_test.shape)\n",
    "\n",
    "meg_0_tensor_train, meg_0_tensor_valid, meg_0_tensor_test = get_splitted_tensor(megsp_list_session_0, megsp_path)\n",
    "meg_1_tensor_train, meg_1_tensor_valid, meg_1_tensor_test = get_splitted_tensor(megsp_list_session_1, megsp_path)\n",
    "meg_tensor_train = torch.cat((meg_0_tensor_train, meg_1_tensor_train), 0)\n",
    "meg_tensor_valid = torch.cat((meg_0_tensor_valid, meg_1_tensor_valid), 0)\n",
    "meg_tensor_test = torch.cat((meg_0_tensor_test, meg_1_tensor_test), 0)\n",
    "print('DIMENSION_MEG_TENSOR_TRAIN: ', meg_tensor_train.shape)\n",
    "print('DIMENSION_MEG_TENSOR_VALID: ', meg_tensor_valid.shape)\n",
    "print('DIMENSION_MEG_TENSOR_TEST: ', meg_tensor_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srv/nfs-data/sisko/matteoc/meg/audio_stft/meg_prediction_ridge_02.pt\n"
     ]
    }
   ],
   "source": [
    "path_to_save = '/srv/nfs-data/sisko/matteoc/meg/audio_stft'\n",
    "print(os.path.join(path_to_save, 'meg_prediction_ridge_'+select_subj+'.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from himalaya.ridge import RidgeCV\n",
    "from himalaya.kernel_ridge import KernelRidgeCV\n",
    "from himalaya.backend import set_backend\n",
    "\n",
    "\n",
    "X_train=audio_tensor_train.cpu().numpy().reshape(audio_tensor_train.shape[0], -1)   \n",
    "y_train=meg_tensor_train.numpy().reshape(meg_tensor_train.shape[0], -1)\n",
    "\n",
    "X_test=audio_tensor_test.cpu().numpy().reshape(audio_tensor_test.shape[0], -1)  \n",
    "y_test=meg_tensor_test.numpy().reshape(meg_tensor_test.shape[0], -1)\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.10, random_state=42, shuffle=True)\n",
    "\n",
    "device_id = 1  # Change this to your desired GPU index\n",
    "torch.cuda.set_device(device_id)  # Set the current device\n",
    "\n",
    "backend = set_backend(\"torch_cuda\")\n",
    "X_train = backend.asarray(X_train).to(f'cuda:{device_id}')\n",
    "y_train = backend.asarray(y_train).to(f'cuda:{device_id}')\n",
    "\n",
    "X_test = backend.asarray(X_test).to(f'cuda:{device_id}')\n",
    "y_test = backend.asarray(y_test).to(f'cuda:{device_id}')\n",
    "\n",
    "# for i in tqdm(range(208)):\n",
    "vm=KernelRidgeCV(alphas=[0.01,1,10,1e2,5e3])\n",
    "vm.fit(X_train,y_train)\n",
    "predict=vm.predict(X_test)\n",
    "voxels_scores=vm.score(X_test,y_test)\n",
    "predict=predict.reshape(X_test.shape[0], 208, -1)\n",
    "y_test_true=y_test.reshape(y_test.shape[0], 208, -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pred_target = []\n",
    "mse_scores = []\n",
    "real_target = []\n",
    "audio_train = audio_tensor_train.reshape(audio_tensor_train.shape[0], -1)\n",
    "audio_test = audio_tensor_test.reshape(audio_tensor_test.shape[0], -1)\n",
    "\n",
    "for channel in tqdm(range(num_channel)):    # 10 canali --> tempo +/- 12 minuti\n",
    "    y_train = meg_tensor_train[:, channel, :, :].reshape(meg_tensor_train.shape[0], -1)\n",
    "    y_test = meg_tensor_test[:, channel, :, :].reshape(meg_tensor_test.shape[0], -1)\n",
    "\n",
    "    # model = RidgeCV(alphas=(5000), cv=(4))\n",
    "    model = Ridge(alpha=5000, max_iter=1000)\n",
    "    model.fit(audio_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(audio_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    pred_target.append(y_pred)\n",
    "    real_target.append(y_test)\n",
    "    mse_scores.append(mse)\n",
    "    print(mse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech-meg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
